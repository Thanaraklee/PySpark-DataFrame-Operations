{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4528f333",
   "metadata": {},
   "source": [
    "# Airline DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "856fa916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in ./.venv/lib/python3.10/site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in ./.venv/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88b5ba3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing pyspark\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "600a7757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing sparksession\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c53aaa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/21 15:34:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#creating a sparksession object and providing appName \n",
    "spark=SparkSession.builder.appName(\"airline\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c227779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create dataframe form External datasets\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"data/airlines1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df456986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', 'Year', 'Quarter', 'Month', 'DayofMonth']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for few columns\n",
    "df.columns[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32edea9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'Year',\n",
       " 'Quarter',\n",
       " 'Month',\n",
       " 'DayofMonth',\n",
       " 'DayOfWeek',\n",
       " 'FlightDate',\n",
       " 'Reporting_Airline',\n",
       " 'DOT_ID_Reporting_Airline',\n",
       " 'IATA_CODE_Reporting_Airline',\n",
       " 'Tail_Number',\n",
       " 'Flight_Number_Reporting_Airline',\n",
       " 'OriginAirportID',\n",
       " 'OriginAirportSeqID',\n",
       " 'OriginCityMarketID',\n",
       " 'Origin',\n",
       " 'OriginCityName',\n",
       " 'OriginState',\n",
       " 'OriginStateFips',\n",
       " 'OriginStateName',\n",
       " 'OriginWac',\n",
       " 'DestAirportID',\n",
       " 'DestAirportSeqID',\n",
       " 'DestCityMarketID',\n",
       " 'Dest',\n",
       " 'DestCityName',\n",
       " 'DestState',\n",
       " 'DestStateFips',\n",
       " 'DestStateName',\n",
       " 'DestWac',\n",
       " 'CRSDepTime',\n",
       " 'DepTime',\n",
       " 'DepDelay',\n",
       " 'DepDelayMinutes',\n",
       " 'DepDel15',\n",
       " 'DepartureDelayGroups',\n",
       " 'DepTimeBlk',\n",
       " 'TaxiOut',\n",
       " 'WheelsOff',\n",
       " 'WheelsOn',\n",
       " 'TaxiIn',\n",
       " 'CRSArrTime',\n",
       " 'ArrTime',\n",
       " 'ArrDelay',\n",
       " 'ArrDelayMinutes',\n",
       " 'ArrDel15',\n",
       " 'ArrivalDelayGroups',\n",
       " 'ArrTimeBlk',\n",
       " 'Cancelled',\n",
       " 'CancellationCode',\n",
       " 'Diverted',\n",
       " 'CRSElapsedTime',\n",
       " 'ActualElapsedTime',\n",
       " 'AirTime',\n",
       " 'Flights',\n",
       " 'Distance',\n",
       " 'DistanceGroup',\n",
       " 'CarrierDelay',\n",
       " 'WeatherDelay',\n",
       " 'NASDelay',\n",
       " 'SecurityDelay',\n",
       " 'LateAircraftDelay',\n",
       " 'FirstDepTime',\n",
       " 'TotalAddGTime',\n",
       " 'LongestAddGTime',\n",
       " 'DivAirportLandings',\n",
       " 'DivReachedDest',\n",
       " 'DivActualElapsedTime',\n",
       " 'DivArrDelay',\n",
       " 'DivDistance',\n",
       " 'Div1Airport',\n",
       " 'Div1AirportID',\n",
       " 'Div1AirportSeqID',\n",
       " 'Div1WheelsOn',\n",
       " 'Div1TotalGTime',\n",
       " 'Div1LongestGTime',\n",
       " 'Div1WheelsOff',\n",
       " 'Div1TailNum',\n",
       " 'Div2Airport',\n",
       " 'Div2AirportID',\n",
       " 'Div2AirportSeqID',\n",
       " 'Div2WheelsOn',\n",
       " 'Div2TotalGTime',\n",
       " 'Div2LongestGTime',\n",
       " 'Div2WheelsOff',\n",
       " 'Div2TailNum',\n",
       " 'Div3Airport',\n",
       " 'Div3AirportID',\n",
       " 'Div3AirportSeqID',\n",
       " 'Div3WheelsOn',\n",
       " 'Div3TotalGTime',\n",
       " 'Div3LongestGTime',\n",
       " 'Div3WheelsOff',\n",
       " 'Div3TailNum',\n",
       " 'Div4Airport',\n",
       " 'Div4AirportID',\n",
       " 'Div4AirportSeqID',\n",
       " 'Div4WheelsOn',\n",
       " 'Div4TotalGTime',\n",
       " 'Div4LongestGTime',\n",
       " 'Div4WheelsOff',\n",
       " 'Div4TailNum',\n",
       " 'Div5Airport',\n",
       " 'Div5AirportID',\n",
       " 'Div5AirportSeqID',\n",
       " 'Div5WheelsOn',\n",
       " 'Div5TotalGTime',\n",
       " 'Div5LongestGTime',\n",
       " 'Div5WheelsOff',\n",
       " 'Div5TailNum']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for all columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0468dd",
   "metadata": {},
   "source": [
    "### Create multiple dataframes as per need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3b861bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/21 15:34:59 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "Flight_Details = df.select(\"_c0\",\"Year\", \"Month\", \"DayofMonth\" , \"FlightDate\",\"Tail_Number\", \"Flight_Number_Reporting_Airline\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6075c29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===========>                                               (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Year|count|\n",
      "+----+-----+\n",
      "|1987|  339|\n",
      "|2016| 1502|\n",
      "|2020|  462|\n",
      "|2012| 1519|\n",
      "|1988| 1310|\n",
      "|2019| 1900|\n",
      "|2017| 1449|\n",
      "|2014| 1513|\n",
      "|2013| 1690|\n",
      "|2005| 1812|\n",
      "|2000| 1477|\n",
      "|2002| 1371|\n",
      "|2009| 1604|\n",
      "|2018| 1862|\n",
      "|1995| 1377|\n",
      "|2006| 1900|\n",
      "|2004| 1847|\n",
      "|2011| 1551|\n",
      "|1989| 1288|\n",
      "|1992| 1335|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count total no of flights in each years \n",
    "Flight_Details.select('Year').groupBy('Year').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4c638ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Month|count|\n",
      "+-----+-----+\n",
      "|    7| 4267|\n",
      "|   11| 3997|\n",
      "|    3| 4396|\n",
      "|    8| 4414|\n",
      "|    5| 4167|\n",
      "|    6| 4078|\n",
      "|    9| 3954|\n",
      "|    1| 4362|\n",
      "|   10| 4264|\n",
      "|    4| 3969|\n",
      "|   12| 4232|\n",
      "|    2| 3901|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count total no of flights in each months\n",
    "Flight_Details.select('Month').groupBy('Month').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d14fccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count total no of cancelled flights in each years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80c6a201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Year|count|\n",
      "+----+-----+\n",
      "|1987|    3|\n",
      "|2016|   17|\n",
      "|2012|   20|\n",
      "|2020|   24|\n",
      "|1988|   15|\n",
      "|2019|   34|\n",
      "|2017|   22|\n",
      "|2014|   31|\n",
      "|2013|   22|\n",
      "|2005|   33|\n",
      "|2000|   42|\n",
      "|2002|   11|\n",
      "|2018|   24|\n",
      "|2009|   19|\n",
      "|1995|   22|\n",
      "|2006|   30|\n",
      "|2004|   29|\n",
      "|2011|   29|\n",
      "|1989|   26|\n",
      "|1992|   14|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Year').filter(\"Cancelled = 1\").groupBy('Year').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "744694ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count total no of cancelled flights in each months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8882c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Month|count|\n",
      "+-----+-----+\n",
      "|    7|   60|\n",
      "|   11|   46|\n",
      "|    3|  106|\n",
      "|    8|   75|\n",
      "|    5|   39|\n",
      "|    6|   60|\n",
      "|    9|  101|\n",
      "|    1|  132|\n",
      "|   10|   57|\n",
      "|    4|   44|\n",
      "|   12|   68|\n",
      "|    2|   90|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Month').filter(\"Cancelled = 1\").groupBy('Month').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1f82734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count total no of flights By departure airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "229eda3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Origin|count|\n",
      "+------+-----+\n",
      "|   BGM|   11|\n",
      "|   INL|    1|\n",
      "|   MSY|  383|\n",
      "|   GEG|  118|\n",
      "|   BUR|  222|\n",
      "|   SNA|  339|\n",
      "|   GRB|   42|\n",
      "|   GTF|   27|\n",
      "|   IDA|   17|\n",
      "|   GRR|  106|\n",
      "|   EUG|   34|\n",
      "|   PSG|   11|\n",
      "|   GSO|  124|\n",
      "|   PVD|  141|\n",
      "|   MYR|   41|\n",
      "|   OAK|  435|\n",
      "|   MSN|   81|\n",
      "|   FAR|   29|\n",
      "|   COD|    3|\n",
      "|   FSM|    5|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Origin').groupBy('Origin').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "144659df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate years wise on time departure flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7400b4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Year|count|\n",
      "+----+-----+\n",
      "|1987|  187|\n",
      "|2016|  978|\n",
      "|2012|  949|\n",
      "|2020|  320|\n",
      "|1988|  775|\n",
      "|2019| 1171|\n",
      "|2017|  911|\n",
      "|2014|  885|\n",
      "|2013| 1011|\n",
      "|2005| 1122|\n",
      "|2000|  812|\n",
      "|2002|  891|\n",
      "|2009|  996|\n",
      "|2018| 1214|\n",
      "|1995|  648|\n",
      "|2006| 1116|\n",
      "|2004| 1226|\n",
      "|2011|  955|\n",
      "|1989|  705|\n",
      "|1992|  721|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Year').filter(\"DepDelay <= 0\").groupBy('Year').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "504d23b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Year|count|\n",
      "+----+-----+\n",
      "|1987|  149|\n",
      "|2016|  507|\n",
      "|2020|  118|\n",
      "|2012|  551|\n",
      "|1988|  520|\n",
      "|2019|  695|\n",
      "|2017|  516|\n",
      "|2014|  597|\n",
      "|2013|  659|\n",
      "|2005|  657|\n",
      "|2000|  623|\n",
      "|2002|  469|\n",
      "|2018|  622|\n",
      "|2009|  590|\n",
      "|1995|  707|\n",
      "|2006|  754|\n",
      "|2004|  592|\n",
      "|2011|  569|\n",
      "|1989|  557|\n",
      "|1992|  600|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#calculate years wise departure delay flights\n",
    "df.select('Year').filter(\"DepDelay > 0\").groupBy('Year').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8aa0417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate years wise permormance of flight like on time departure, on time arrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adbd6036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Year|count|\n",
      "+----+-----+\n",
      "|1987|   96|\n",
      "|2016|  840|\n",
      "|2012|  782|\n",
      "|2020|  280|\n",
      "|1988|  474|\n",
      "|2019|  982|\n",
      "|2017|  739|\n",
      "|2014|  726|\n",
      "|2013|  829|\n",
      "|2005|  845|\n",
      "|2000|  579|\n",
      "|2002|  695|\n",
      "|2009|  793|\n",
      "|2018| 1032|\n",
      "|1995|  444|\n",
      "|2006|  821|\n",
      "|2004|  921|\n",
      "|1989|  453|\n",
      "|2011|  757|\n",
      "|1992|  501|\n",
      "+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('Year').filter((df.DepDelay <= 0) & (df.ArrDelay <= 0)).groupBy('Year').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901f5981",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9da294cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_Origin= df.select(\"_c0\",\"OriginAirportID\", \"OriginAirportSeqID\", \"OriginCityMarketID\",\"Origin\", \"OriginCityName\",\"OriginState\", \"OriginStateName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55c8b3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_Destination = df.select(\"_c0\",\"DestAirportID\", \"DestAirportSeqID\", \"Dest\", \"DestCityName\",\"DestState\",\"DestStateName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89d462bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_arr_dep = df.select(\"_c0\",\"CRSArrTime\",\"ArrTime\", \"CRSDepTime\",\"DepTime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca1df19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DelayDF= df.select(\"_c0\",\"CarrierDelay\",\"WeatherDelay\", \"NASDelay\",\"SecurityDelay\",\"LateAircraftDelay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9cbfe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_cancel = df.select(\"_c0\",\"Cancelled\",\"CancellationCode\",\"Diverted\",\"Flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f47c9b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify column name\n",
    "# to modify multiple column name\n",
    "#Flight_cancel.withColumnRenamed(\"_c0\",\"ID\").withColumnRenamed(\"CancellationCode\",\"Code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ea838fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "Flight_cancel = Flight_cancel.withColumnRenamed(\"_c0\",\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fa4ace09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------------+--------+-------+\n",
      "| ID|Cancelled|CancellationCode|Diverted|Flights|\n",
      "+---+---------+----------------+--------+-------+\n",
      "|  0|      0.0|            null|     0.0|    1.0|\n",
      "|  1|      0.0|            null|     0.0|    1.0|\n",
      "|  2|      0.0|            null|     0.0|    1.0|\n",
      "|  3|      0.0|            null|     0.0|    1.0|\n",
      "|  4|      0.0|            null|     0.0|    1.0|\n",
      "|  5|      0.0|            null|     0.0|    1.0|\n",
      "|  6|      0.0|            null|     0.0|    1.0|\n",
      "|  7|      0.0|            null|     0.0|    1.0|\n",
      "|  8|      0.0|            null|     0.0|    1.0|\n",
      "|  9|      0.0|            null|     0.0|    1.0|\n",
      "| 10|      0.0|            null|     0.0|    1.0|\n",
      "| 11|      0.0|            null|     0.0|    1.0|\n",
      "| 12|      0.0|            null|     0.0|    1.0|\n",
      "| 13|      0.0|            null|     0.0|    1.0|\n",
      "| 14|      0.0|            null|     0.0|    1.0|\n",
      "| 15|      0.0|            null|     0.0|    1.0|\n",
      "| 16|      0.0|            null|     0.0|    1.0|\n",
      "| 17|      0.0|            null|     0.0|    1.0|\n",
      "| 18|      0.0|            null|     0.0|    1.0|\n",
      "| 19|      1.0|               A|     0.0|    1.0|\n",
      "+---+---------+----------------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/21 15:35:43 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , Cancelled, CancellationCode, Diverted, Flights\n",
      " Schema: _c0, Cancelled, CancellationCode, Diverted, Flights\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/thanarak/Desktop/PySpark-DataFrame-Operations/data/airlines1.csv\n"
     ]
    }
   ],
   "source": [
    "Flight_cancel.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ee3b21",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "handling null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a1030b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------------+--------+-------+\n",
      "| ID|Cancelled|CancellationCode|Diverted|Flights|\n",
      "+---+---------+----------------+--------+-------+\n",
      "|  0|      0.0|            null|     0.0|    1.0|\n",
      "|  1|      0.0|            null|     0.0|    1.0|\n",
      "|  2|      0.0|            null|     0.0|    1.0|\n",
      "|  3|      0.0|            null|     0.0|    1.0|\n",
      "|  4|      0.0|            null|     0.0|    1.0|\n",
      "|  5|      0.0|            null|     0.0|    1.0|\n",
      "|  6|      0.0|            null|     0.0|    1.0|\n",
      "|  7|      0.0|            null|     0.0|    1.0|\n",
      "|  8|      0.0|            null|     0.0|    1.0|\n",
      "|  9|      0.0|            null|     0.0|    1.0|\n",
      "| 10|      0.0|            null|     0.0|    1.0|\n",
      "| 11|      0.0|            null|     0.0|    1.0|\n",
      "| 12|      0.0|            null|     0.0|    1.0|\n",
      "| 13|      0.0|            null|     0.0|    1.0|\n",
      "| 14|      0.0|            null|     0.0|    1.0|\n",
      "| 15|      0.0|            null|     0.0|    1.0|\n",
      "| 16|      0.0|            null|     0.0|    1.0|\n",
      "| 17|      0.0|            null|     0.0|    1.0|\n",
      "| 18|      0.0|            null|     0.0|    1.0|\n",
      "| 19|      1.0|               A|     0.0|    1.0|\n",
      "+---+---------+----------------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/21 15:35:44 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , Cancelled, CancellationCode, Diverted, Flights\n",
      " Schema: _c0, Cancelled, CancellationCode, Diverted, Flights\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/thanarak/Desktop/PySpark-DataFrame-Operations/data/airlines1.csv\n"
     ]
    }
   ],
   "source": [
    "Flight_cancel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "463eba12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----------------+--------+-------+\n",
      "|  ID|Cancelled|CancellationCode|Diverted|Flights|\n",
      "+----+---------+----------------+--------+-------+\n",
      "|  19|      1.0|               A|     0.0|    1.0|\n",
      "| 146|      1.0|               B|     0.0|    1.0|\n",
      "| 197|      1.0|               C|     0.0|    1.0|\n",
      "| 554|      1.0|               A|     0.0|    1.0|\n",
      "| 680|      1.0|               A|     0.0|    1.0|\n",
      "| 841|      1.0|               A|     0.0|    1.0|\n",
      "| 905|      1.0|               B|     0.0|    1.0|\n",
      "| 911|      1.0|               B|     0.0|    1.0|\n",
      "| 965|      1.0|               B|     0.0|    1.0|\n",
      "|1082|      1.0|               C|     0.0|    1.0|\n",
      "|1246|      1.0|               A|     0.0|    1.0|\n",
      "|1443|      1.0|               B|     0.0|    1.0|\n",
      "|1450|      1.0|               A|     0.0|    1.0|\n",
      "|1633|      1.0|               A|     0.0|    1.0|\n",
      "|1730|      1.0|               B|     0.0|    1.0|\n",
      "|1773|      1.0|               C|     0.0|    1.0|\n",
      "|1904|      1.0|               B|     0.0|    1.0|\n",
      "|1908|      1.0|               B|     0.0|    1.0|\n",
      "|1913|      1.0|               A|     0.0|    1.0|\n",
      "|2450|      1.0|               A|     0.0|    1.0|\n",
      "+----+---------+----------------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/21 15:35:44 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , Cancelled, CancellationCode, Diverted, Flights\n",
      " Schema: _c0, Cancelled, CancellationCode, Diverted, Flights\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/thanarak/Desktop/PySpark-DataFrame-Operations/data/airlines1.csv\n"
     ]
    }
   ],
   "source": [
    "Flight_cancel.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "037b28d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+----------------+--------+-------+\n",
      "|  ID|Cancelled|CancellationCode|Diverted|Flights|\n",
      "+----+---------+----------------+--------+-------+\n",
      "|  19|      1.0|               A|     0.0|    1.0|\n",
      "| 146|      1.0|               B|     0.0|    1.0|\n",
      "| 197|      1.0|               C|     0.0|    1.0|\n",
      "| 554|      1.0|               A|     0.0|    1.0|\n",
      "| 680|      1.0|               A|     0.0|    1.0|\n",
      "| 841|      1.0|               A|     0.0|    1.0|\n",
      "| 905|      1.0|               B|     0.0|    1.0|\n",
      "| 911|      1.0|               B|     0.0|    1.0|\n",
      "| 965|      1.0|               B|     0.0|    1.0|\n",
      "|1082|      1.0|               C|     0.0|    1.0|\n",
      "|1246|      1.0|               A|     0.0|    1.0|\n",
      "|1443|      1.0|               B|     0.0|    1.0|\n",
      "|1450|      1.0|               A|     0.0|    1.0|\n",
      "|1633|      1.0|               A|     0.0|    1.0|\n",
      "|1730|      1.0|               B|     0.0|    1.0|\n",
      "|1773|      1.0|               C|     0.0|    1.0|\n",
      "|1904|      1.0|               B|     0.0|    1.0|\n",
      "|1908|      1.0|               B|     0.0|    1.0|\n",
      "|1913|      1.0|               A|     0.0|    1.0|\n",
      "|2450|      1.0|               A|     0.0|    1.0|\n",
      "+----+---------+----------------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/21 15:35:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , Cancelled, CancellationCode, Diverted, Flights\n",
      " Schema: _c0, Cancelled, CancellationCode, Diverted, Flights\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/thanarak/Desktop/PySpark-DataFrame-Operations/data/airlines1.csv\n"
     ]
    }
   ],
   "source": [
    "Flight_cancel.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e8a9168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------------+--------+-------+\n",
      "| ID|Cancelled|CancellationCode|Diverted|Flights|\n",
      "+---+---------+----------------+--------+-------+\n",
      "|  0|      0.0|             yes|     0.0|    1.0|\n",
      "|  1|      0.0|             yes|     0.0|    1.0|\n",
      "|  2|      0.0|             yes|     0.0|    1.0|\n",
      "|  3|      0.0|             yes|     0.0|    1.0|\n",
      "|  4|      0.0|             yes|     0.0|    1.0|\n",
      "|  5|      0.0|             yes|     0.0|    1.0|\n",
      "|  6|      0.0|             yes|     0.0|    1.0|\n",
      "|  7|      0.0|             yes|     0.0|    1.0|\n",
      "|  8|      0.0|             yes|     0.0|    1.0|\n",
      "|  9|      0.0|             yes|     0.0|    1.0|\n",
      "| 10|      0.0|             yes|     0.0|    1.0|\n",
      "| 11|      0.0|             yes|     0.0|    1.0|\n",
      "| 12|      0.0|             yes|     0.0|    1.0|\n",
      "| 13|      0.0|             yes|     0.0|    1.0|\n",
      "| 14|      0.0|             yes|     0.0|    1.0|\n",
      "| 15|      0.0|             yes|     0.0|    1.0|\n",
      "| 16|      0.0|             yes|     0.0|    1.0|\n",
      "| 17|      0.0|             yes|     0.0|    1.0|\n",
      "| 18|      0.0|             yes|     0.0|    1.0|\n",
      "| 19|      1.0|               A|     0.0|    1.0|\n",
      "+---+---------+----------------+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/21 15:35:45 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , Cancelled, CancellationCode, Diverted, Flights\n",
      " Schema: _c0, Cancelled, CancellationCode, Diverted, Flights\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/thanarak/Desktop/PySpark-DataFrame-Operations/data/airlines1.csv\n"
     ]
    }
   ],
   "source": [
    "Flight_cancel.na.fill(\"yes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc7167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79c09bff",
   "metadata": {},
   "source": [
    "## apply pyspark windows function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5cc13b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8372f92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/21 15:35:46 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , OriginAirportID, OriginAirportSeqID, OriginCityMarketID, Origin, OriginCityName, OriginState, OriginStateName\n",
      " Schema: _c0, OriginAirportID, OriginAirportSeqID, OriginCityMarketID, Origin, OriginCityName, OriginState, OriginStateName\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/thanarak/Desktop/PySpark-DataFrame-Operations/data/airlines1.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+------------------+------------------+------+--------------------+-----------+---------------+----------+\n",
      "|  _c0|OriginAirportID|OriginAirportSeqID|OriginCityMarketID|Origin|      OriginCityName|OriginState|OriginStateName|row_number|\n",
      "+-----+---------------+------------------+------------------+------+--------------------+-----------+---------------+----------+\n",
      "|22201|          12016|           1201601|             32016|   GUM|          Guam, Guam|       null|           null|         1|\n",
      "|32252|          12016|           1201601|             32016|   GUM|          Guam, Guam|       null|           null|         2|\n",
      "|47945|          14583|           1458301|             34583|   ROR|Koror, Palau Dist...|       null|           null|         3|\n",
      "| 4406|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|         4|\n",
      "|11056|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|         5|\n",
      "|11127|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|         6|\n",
      "|17268|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|         7|\n",
      "|19012|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|         8|\n",
      "|20504|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|         9|\n",
      "|21724|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|        10|\n",
      "|27784|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|        11|\n",
      "|31525|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|        12|\n",
      "|39618|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|        13|\n",
      "|18173|          14955|           1495501|             34955|   SPN|Saipan, Northern ...|       null|           null|        14|\n",
      "|22928|          15024|           1502402|             34945|   STT|Charlotte Amalie,...|       null|           null|        15|\n",
      "|27212|          15024|           1502402|             34945|   STT|Charlotte Amalie,...|       null|           null|        16|\n",
      "|29420|          15024|           1502402|             34945|   STT|Charlotte Amalie,...|       null|           null|        17|\n",
      "|37516|          15024|           1502402|             34945|   STT|Charlotte Amalie,...|       null|           null|        18|\n",
      "|40192|          15024|           1502402|             34945|   STT|Charlotte Amalie,...|       null|           null|        19|\n",
      "|45635|          15027|           1502701|             34992|   STX|Christiansted, Vi...|       null|           null|        20|\n",
      "+-----+---------------+------------------+------------------+------+--------------------+-----------+---------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ndf  = Window.partitionBy(\"OriginState\").orderBy(\"OriginAirportID\")\n",
    "Flight_Origin.withColumn(\"row_number\",row_number().over(ndf)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "566c38c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/21 15:35:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , OriginAirportID, OriginAirportSeqID, OriginCityMarketID, Origin, OriginCityName, OriginState, OriginStateName\n",
      " Schema: _c0, OriginAirportID, OriginAirportSeqID, OriginCityMarketID, Origin, OriginCityName, OriginState, OriginStateName\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/thanarak/Desktop/PySpark-DataFrame-Operations/data/airlines1.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+------------------+------------------+------+--------------------+-----------+---------------+----+\n",
      "|  _c0|OriginAirportID|OriginAirportSeqID|OriginCityMarketID|Origin|      OriginCityName|OriginState|OriginStateName|rank|\n",
      "+-----+---------------+------------------+------------------+------+--------------------+-----------+---------------+----+\n",
      "|22201|          12016|           1201601|             32016|   GUM|          Guam, Guam|       null|           null|   1|\n",
      "|32252|          12016|           1201601|             32016|   GUM|          Guam, Guam|       null|           null|   1|\n",
      "|47945|          14583|           1458301|             34583|   ROR|Koror, Palau Dist...|       null|           null|   3|\n",
      "| 4406|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|   4|\n",
      "|11056|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|   4|\n",
      "|11127|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|   4|\n",
      "|17268|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|   4|\n",
      "|19012|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|   4|\n",
      "|20504|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|   4|\n",
      "|21724|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|   4|\n",
      "|27784|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|   4|\n",
      "|31525|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|   4|\n",
      "|39618|          14843|           1484301|             34819|   SJU|San Juan, Puerto ...|       null|           null|   4|\n",
      "|18173|          14955|           1495501|             34955|   SPN|Saipan, Northern ...|       null|           null|  14|\n",
      "|22928|          15024|           1502402|             34945|   STT|Charlotte Amalie,...|       null|           null|  15|\n",
      "|27212|          15024|           1502402|             34945|   STT|Charlotte Amalie,...|       null|           null|  15|\n",
      "|29420|          15024|           1502402|             34945|   STT|Charlotte Amalie,...|       null|           null|  15|\n",
      "|37516|          15024|           1502402|             34945|   STT|Charlotte Amalie,...|       null|           null|  15|\n",
      "|40192|          15024|           1502402|             34945|   STT|Charlotte Amalie,...|       null|           null|  15|\n",
      "|45635|          15027|           1502701|             34992|   STX|Christiansted, Vi...|       null|           null|  20|\n",
      "+-----+---------------+------------------+------------------+------+--------------------+-----------+---------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use rank function\n",
    "from pyspark.sql.functions import rank\n",
    "Flight_Origin.withColumn(\"rank\",rank().over(ndf)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce597509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+------------------+------------------+------+--------------------+-----------+---------------+\n",
      "|  _c0|OriginAirportID|OriginAirportSeqID|OriginCityMarketID|Origin|      OriginCityName|OriginState|OriginStateName|\n",
      "+-----+---------------+------------------+------------------+------+--------------------+-----------+---------------+\n",
      "|46502|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|38459|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|23780|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|39459|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "| 1590|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|40523|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|24002|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|42080|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|12518|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|42910|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|24780|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|44533|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "| 5113|          10135|           1013502|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|45933|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|25125|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|46998|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|25502|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "| 5403|          10135|           1013505|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|26066|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|13333|          10135|           1013505|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "+-----+---------------+------------------+------------------+------+--------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/21 15:35:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , OriginAirportID, OriginAirportSeqID, OriginCityMarketID, Origin, OriginCityName, OriginState, OriginStateName\n",
      " Schema: _c0, OriginAirportID, OriginAirportSeqID, OriginCityMarketID, Origin, OriginCityName, OriginState, OriginStateName\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/thanarak/Desktop/PySpark-DataFrame-Operations/data/airlines1.csv\n"
     ]
    }
   ],
   "source": [
    "# sorting dataframe by using sort\n",
    "Flight_Origin.sort(\"Origin\",\"OriginState\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "34a60f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+------------------+------------------+------+--------------------+-----------+---------------+\n",
      "|  _c0|OriginAirportID|OriginAirportSeqID|OriginCityMarketID|Origin|      OriginCityName|OriginState|OriginStateName|\n",
      "+-----+---------------+------------------+------------------+------+--------------------+-----------+---------------+\n",
      "|46502|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|23780|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "| 1590|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|24002|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|12518|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|24780|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "| 5113|          10135|           1013502|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|25125|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|38459|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|25502|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "| 5403|          10135|           1013505|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|26066|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|13333|          10135|           1013505|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|26118|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "| 5819|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|27478|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|46998|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|27701|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "| 6277|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "|28914|          10135|           1013501|             30135|   ABE|Allentown/Bethleh...|         PA|   Pennsylvania|\n",
      "+-----+---------------+------------------+------------------+------+--------------------+-----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/21 15:35:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , OriginAirportID, OriginAirportSeqID, OriginCityMarketID, Origin, OriginCityName, OriginState, OriginStateName\n",
      " Schema: _c0, OriginAirportID, OriginAirportSeqID, OriginCityMarketID, Origin, OriginCityName, OriginState, OriginStateName\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/thanarak/Desktop/PySpark-DataFrame-Operations/data/airlines1.csv\n"
     ]
    }
   ],
   "source": [
    "# sorting datafranme by orderby\n",
    "Flight_Origin.orderBy(\"Origin\",\"OriginState\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac71547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by ascending\n",
    "#Flight_Origin.sort(Flight_Origin.Origin.asc(),df.OriginState.asc()).show()\n",
    "#Flight_Origin.orderBy(col(\"Origin\").asc(),col(\"OriginState\").asc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82ba95bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by desciending \n",
    "#Flight_Origin.sort(Flight_Origin.Origin.desc(),df.OriginState.desc()).show()\n",
    "#Flight_Origin.orderBy(col(\"Origin\").desc(),col(\"OriginState\").desc()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2995b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType \n",
    "from pyspark.sql.types import ArrayType, DoubleType, BooleanType\n",
    "from pyspark.sql.functions import col,array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af2dcbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------------+----+--------------------+---------+--------------+\n",
      "|_c0|DestAirportID|DestAirportSeqID|Dest|        DestCityName|DestState| DestStateName|\n",
      "+---+-------------+----------------+----+--------------------+---------+--------------+\n",
      "|  0|        14869|         1486902| SLC|  Salt Lake City, UT|       UT|          Utah|\n",
      "|  1|        13204|         1320401| MCO|         Orlando, FL|       FL|       Florida|\n",
      "|  2|        11298|         1129803| DFW|Dallas/Fort Worth...|       TX|         Texas|\n",
      "|  3|        11433|         1143301| DTW|         Detroit, MI|       MI|      Michigan|\n",
      "|  4|        11057|         1105702| CLT|       Charlotte, NC|       NC|North Carolina|\n",
      "|  5|        14814|         1481401| SHV|      Shreveport, LA|       LA|     Louisiana|\n",
      "|  6|        11042|         1104201| CLE|       Cleveland, OH|       OH|          Ohio|\n",
      "|  7|        10868|         1086803| CAE|        Columbia, SC|       SC|South Carolina|\n",
      "|  8|        11042|         1104201| CLE|       Cleveland, OH|       OH|          Ohio|\n",
      "|  9|        11259|         1125903| DAL|          Dallas, TX|       TX|         Texas|\n",
      "| 10|        12892|         1289201| LAX|     Los Angeles, CA|       CA|    California|\n",
      "| 11|        11259|         1125903| DAL|          Dallas, TX|       TX|         Texas|\n",
      "| 12|        13303|         1330302| MIA|           Miami, FL|       FL|       Florida|\n",
      "| 13|        12892|         1289203| LAX|     Los Angeles, CA|       CA|    California|\n",
      "| 14|        12953|         1295301| LGA|        New York, NY|       NY|      New York|\n",
      "| 15|        10821|         1082102| BWI|       Baltimore, MD|       MD|      Maryland|\n",
      "| 16|        13184|         1318402| MBS|Saginaw/Bay City/...|       MI|      Michigan|\n",
      "| 17|        14747|         1474703| SEA|         Seattle, WA|       WA|    Washington|\n",
      "| 18|        12264|         1226401| IAD|      Washington, DC|       VA|      Virginia|\n",
      "| 19|        12892|         1289205| LAX|     Los Angeles, CA|       CA|    California|\n",
      "+---+-------------+----------------+----+--------------------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/21 15:35:48 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , DestAirportID, DestAirportSeqID, Dest, DestCityName, DestState, DestStateName\n",
      " Schema: _c0, DestAirportID, DestAirportSeqID, Dest, DestCityName, DestState, DestStateName\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/thanarak/Desktop/PySpark-DataFrame-Operations/data/airlines1.csv\n"
     ]
    }
   ],
   "source": [
    "Flight_Destination.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4fbe0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "164ba339",
   "metadata": {},
   "source": [
    "## how to use UDF in pyspark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "661b9ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eff98d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad7abd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.types import StructType, IntegerType, StringType\n",
    "# as per datatype StructType, IntegerType, StringType in columns we need to import these library \n",
    "#otherwise bydefaut it wil take StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "68ac5587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65ac9a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def destination(str):\n",
    "    resStr=\"\"\n",
    "    arr = str.split(\",\")\n",
    "    for x in arr:\n",
    "       resStr= arr[0]\n",
    "    return resStr \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce93f94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting python function to UDF\n",
    "destinationUDF = udf(lambda z: destination(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8a1d8b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------+\n",
      "|Dest|DestCityName            |\n",
      "+----+------------------------+\n",
      "|SLC |Salt Lake City          |\n",
      "|MCO |Orlando                 |\n",
      "|DFW |Dallas/Fort Worth       |\n",
      "|DTW |Detroit                 |\n",
      "|CLT |Charlotte               |\n",
      "|SHV |Shreveport              |\n",
      "|CLE |Cleveland               |\n",
      "|CAE |Columbia                |\n",
      "|CLE |Cleveland               |\n",
      "|DAL |Dallas                  |\n",
      "|LAX |Los Angeles             |\n",
      "|DAL |Dallas                  |\n",
      "|MIA |Miami                   |\n",
      "|LAX |Los Angeles             |\n",
      "|LGA |New York                |\n",
      "|BWI |Baltimore               |\n",
      "|MBS |Saginaw/Bay City/Midland|\n",
      "|SEA |Seattle                 |\n",
      "|IAD |Washington              |\n",
      "|LAX |Los Angeles             |\n",
      "+----+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(col(\"Dest\"),destinationUDF(col(\"DestCityName\")).alias(\"DestCityName\") ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "80805ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------+----------------+----+--------------------+---------+--------------+\n",
      "|_c0|DestAirportID|DestAirportSeqID|Dest|        DestCityName|DestState| DestStateName|\n",
      "+---+-------------+----------------+----+--------------------+---------+--------------+\n",
      "|  0|        14869|         1486902| SLC|  Salt Lake City, UT|       UT|          Utah|\n",
      "|  1|        13204|         1320401| MCO|         Orlando, FL|       FL|       Florida|\n",
      "|  2|        11298|         1129803| DFW|Dallas/Fort Worth...|       TX|         Texas|\n",
      "|  3|        11433|         1143301| DTW|         Detroit, MI|       MI|      Michigan|\n",
      "|  4|        11057|         1105702| CLT|       Charlotte, NC|       NC|North Carolina|\n",
      "|  5|        14814|         1481401| SHV|      Shreveport, LA|       LA|     Louisiana|\n",
      "|  6|        11042|         1104201| CLE|       Cleveland, OH|       OH|          Ohio|\n",
      "|  7|        10868|         1086803| CAE|        Columbia, SC|       SC|South Carolina|\n",
      "|  8|        11042|         1104201| CLE|       Cleveland, OH|       OH|          Ohio|\n",
      "|  9|        11259|         1125903| DAL|          Dallas, TX|       TX|         Texas|\n",
      "| 10|        12892|         1289201| LAX|     Los Angeles, CA|       CA|    California|\n",
      "| 11|        11259|         1125903| DAL|          Dallas, TX|       TX|         Texas|\n",
      "| 12|        13303|         1330302| MIA|           Miami, FL|       FL|       Florida|\n",
      "| 13|        12892|         1289203| LAX|     Los Angeles, CA|       CA|    California|\n",
      "| 14|        12953|         1295301| LGA|        New York, NY|       NY|      New York|\n",
      "| 15|        10821|         1082102| BWI|       Baltimore, MD|       MD|      Maryland|\n",
      "| 16|        13184|         1318402| MBS|Saginaw/Bay City/...|       MI|      Michigan|\n",
      "| 17|        14747|         1474703| SEA|         Seattle, WA|       WA|    Washington|\n",
      "| 18|        12264|         1226401| IAD|      Washington, DC|       VA|      Virginia|\n",
      "| 19|        12892|         1289205| LAX|     Los Angeles, CA|       CA|    California|\n",
      "+---+-------------+----------------+----+--------------------+---------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/21 15:35:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , DestAirportID, DestAirportSeqID, Dest, DestCityName, DestState, DestStateName\n",
      " Schema: _c0, DestAirportID, DestAirportSeqID, Dest, DestCityName, DestState, DestStateName\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///home/thanarak/Desktop/PySpark-DataFrame-Operations/data/airlines1.csv\n"
     ]
    }
   ],
   "source": [
    "Flight_Destination.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae0579bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8b119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
